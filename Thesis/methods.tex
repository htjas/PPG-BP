In the Methods section are summarized the methods of this project.
The project is named \texttt{PPG-BP} having a couple of sub-directories, such as the MIMIC, or PyTorch tutorials, but the main code can be found in the \texttt{model} Python Package.
In the \texttt{model}, there are several other sub-packages, mostly containing relevant data used for further parts of the project, but the main code is written in 5 \texttt{.py} classes:
\texttt{init\_scripts.py} (Initialization / Data Fetching), \texttt{sp\_scripts.py} (Signal Processing), \texttt{ml\_scripts.py} (Machine Learning), \texttt{visual.py} (Visualization) and \texttt{main.py} - Integrating all the before mentioned components.

The whole repository can be found on the author's \textit{GitHub} page~\cite{jasinskasHtjasPPGBP2024}.

\subsection{Data Fetching}
\label{subsec:data-fetching}

The first step in virtually all Data Science projects is the Data Preparation.
The \texttt{init\_scripts.py} class will be looked at in this section.
In this project the data was gathered from the MIMIC-III and MIMIC-IV DBs.
They were used for different purposes, more to be seen in later chapters.
However the fetching of the data involved identical approaches from both the newer and the older waveform datasets.

% Libraries
\vspace{0.2cm}
\textit{Tools and Approaches}
\vspace{0.2cm}

Researchers at the MIT Laboratory for Computational Physiology have develop a native Python library for convenient handling of MIMIC datasets, called the waveform-database (WFDB) package.
A library of tools for reading, writing, and processing WFDB signals and annotations~\cite{MITLCPWfdbpython2024}.
A compilation of pre-written WFDB library and self-written methods were used for efficient and consistent data fetching.

First of all, a list of all of the records from the selected DB were fetched utilizing the

\vspace{0.1cm}
{\centering \texttt{wfdb.get\_record\_list(db\_name)}\par}
\vspace{0.1cm}

\noindent method.
Then an iterative process followed, going through all of the subjects from the loaded records list.
When assessing a single subject, the method

\vspace{0.1cm}
{\centering \texttt{wfdb.get\_record\_list((f'{db\_name}/{subject}')}\par}
\vspace{0.1cm}

\noindent was called, in this instance to load all of the studies from a single subject.
Another loop was formed in order to go through every study of the subject.
These studies are made up of records, each one representing a single ICU \enquote{session}, when the patient was linked up to at least one monitoring device, therefore they highly differ in time length.
Due to the diversity of monitoring devices, these records contain various signals, such as \textit{ECG}, \textit{ABP}, \textit{Pleth} (the equivalent of PPG).
To check whether the record contains all required signals (in the case of this research the signals \textit{ABP} and \textit{Pleth} were the necessary condition),
the method

\vspace{0.1cm}
{\centering \texttt{record\_data = wfdb.rdheader(subject.name, record\_dir, rd\_segments=True)}\par}
\vspace{0.1cm}

\noindent was called, providing the header data of a single record,
which was then used to fetch all present signals

\vspace{0.1cm}
{\centering \texttt{signal\_names = record\_data.sig\_name}.\par}
\vspace{0.1cm}

\noindent If the record did not contain both \textit{ABP} and \textit{Pleth} signals, then it was skipped.
However, even a single record contains various segments, that may also differ in lengths and recorded signals.
For the selection of the usable segments, in addition to the required signals, another time-based requirement was added, namely that only those segments with at least 10 minutes of length were considered.
This decision was made, in order to account for data reliability and variability.
A longer duration allows for capturing sufficient data points to represent various physiological states accurately, reducing the influence of short-term fluctuations.
Secondly, longer segments mitigate the impact of noise and artifacts, providing a more reliable and stable signal for analysis.
The length of a segment is calculated by the following function:

\vspace{0.1cm}
\qquad\qquad\qquad \texttt{segment\_metadata = wfdb.rdheader(segment, record\_dir)}

\qquad\qquad\qquad \texttt{tot\_seg\_length = segment\_metadata.sig\_len}

\qquad\qquad\qquad \texttt{sampling\_rate = segment\_metadata.fs}

\qquad\qquad\qquad \texttt{seg\_duration = tot\_seg\_length / fs}
\vspace{0.1cm}

Precisely these segments are the smallest data samples, that are used in this research, since they contain continuous numeric values at a specific sampling rate.
The abstraction level of data points stored in MIMIC DBs can be visualized by this chart from left to right in descending order:

\vspace{0.1cm}
{\centering \textit{Subject} \rightarrow Study \rightarrow Record \rightarrow Segment\par}
\vspace{0.1cm}

An iterative process is likewise applied to every segment of a record, by fetching all values from a single segment:

\vspace{0.1cm}
{\centering \texttt{segment\_data = wfdb.rdrecord(segment, record\_dir)}.\par}
\vspace{0.1cm}

\noindent These segments must also not contain any \enquote{faulty} values.
As faulty are regarded such values: \texttt{NaN}, \texttt{inf}, for ABP - values not below 30 and not above 250 (accounting for extreme and most likely wrongly measured values), for Pleth only between 0 and 1 (precise valid PPG range).
The exclusion criteria are summarized in the following table.

\begin{center}
    \begin{tabular}{ |c|c| }
        \hline
        ABP              & Pleth         \\
        \hline
        30 $<$ x $<$ 250 & 0 $<$ x $<$ 1 \\
        \hline
        \multicolumn{2}{|c|}{NaN, inf} \\
        \hline
    \end{tabular}
\end{center}

If a 10-minute part of the segment matches these criteria, it is saved to the ./validation directory, as 2 separate and synchronous ABP and PPG files
If a segment is longer than 10 minutes, then more 10 minute segments may be extracted (marked by a \_ and a number representing the sequence)
Example of the files: \texttt{abp\_80057524\_0005\_0}, \texttt{ppg\_80057524\_0020\_18}

For MIMIC-IV data fetching, no limits on the total records and single study records are given.
For MIMIC-III - 4 times the records of MIMIC-IV are fetched and a maximum of 100 segments per study is set, to avoid over-representation of single person or study data in the ML part of the research.

\begin{verbatim}
    wfdb.get_record_list(db_name)
    wfdb.rdheader()
    wfdb.rdrecord()
\end{verbatim}

% CODE in LATEX EXAMPLE
%\begin{lstlisting}[language=Python,label={lst:python}]
%    import wfdb
%
%    wfdb.get_record_list(db_name)
%    wfdb.rdrecord()
%    wfdb.rdheader()
%\end{lstlisting}


elaborate record filtering criteria

\subsection{Signal Processing}
\label{subsec:signal-processing}

\subsubsection{Filtering Aproaches}

butter + savgol optimal, because PPG shapes best (show pic)

\subsubsection{Beat Finding Algorithms}

3 beat findings algos from wfdb methods, 1 novel for reference

Beat grouping and synchronization

Mention that no manual selection was done, due to large size of samples

\subsubsection{Fiducial Point detection}

wfdb default algos

cave for dic notch

\subsubsection{Feature Extraction}

graphics for all 34 features

explain each significance

FFT formula

\subsection{Machine Learning}\label{subsec:ml_methods}

LR, MLP, LSTM, GRU models tried

elaborate splitting approach

\begin{enumerate}

    \item Data fetching and reading using WFDB and NumPy libraries.

    \subitem Get all available data from MIMIC-IV DB.\
    \subitem This data is used as the validation dataset, since it is the one that's most up-to-date,
    most modern, and supposedly most reliable
    \subitem There are 198 subjects and 200 total studies in the DB (two subjects have two separate studies)
    \subitem The studies contain records with various signals (ECG, ABP, Pleth etc.) and are of different lengths
    \subitem A single record can contain various segments also of different lengths and signals
    \subitem Segments contain continuous values, that are used in this research
    \subitem Only segments with at least 10 minutes of length are considered
    \subitem These segments must also not contain \enquote{faulty} values
    \subitem Faulty values: NaN, inf, negative values, for ABP values not below 30 and not above 250,
    for PPG only between 0 and 1
    \subitem If a 10-minute part of the segment matches these criteria, it is saved to the ./validation directory,
    as 2 separate and synchronous ABP and PPG files
    \subitem If a segment is longer than 10 minutes, then more 10 minute segments may be extracted
    (marked by a \_ and a number representing the sequence)
    \subitem Example of the files: \texttt{abp\_80057524\_0005\_0}, \texttt{ppg\_80057524\_0020\_18}

    For MIMIC-IV data fetching, no limits on the total records and single study records are given.
    For MIMIC-III - 4 times the records of MIMIC-IV are fetched and a maximum of 100 segments per study is set,
    to avoid over-representation of single person or study data in the ML part of the research.

    \item Digital signal filtering using Savitzky-Golay and Butterworth Lowpass filters.

    \item Beat detection algorithms from MIMIC WFDB Tutorial used for primary estimation.
    Beat detection improved with manual implementation of SciPy library.

    \item Fiducial Point calculation based on the algorithm provided in the MIMIC WFDB Tutorial.

    \item Feature extraction with personally created code to extract time domain features, and NumPy library to extract frequency domain features (FFT). Median value calculation using NumPy library.

    \item Machine Learning Model creation using PyTorch and Scikit-Learn libraries.

\end{enumerate}
